{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## Comparing models for our different tasks\n",
    "\n",
    "In this Notebook, we are going to use another model, Flan-T5-Small in parallel to Mistral-7B and see how it behaves.\n",
    "\n",
    "Flan-T5-Small is indeed much smaller, runs without GPU and on only 1GB of RAM, but is it up to the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt # Uncomment only if you have not selected the right workbench image\n",
    "\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain pipeline\n",
    "\n",
    "We are now going to define two different LLM endpoints, and two different pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main LLM Inference Server URL\n",
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "782046b7-f0a4-487c-86fc-3131e668c6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Flan-T5-Small LLM Inference Server URL\n",
    "inference_server_url_flan_t5 = \"http://llm-flant5.ic-shared-llm.svc.cluster.local:3000/\"\n",
    "\n",
    "# LLM definition\n",
    "llm_flant5 = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url_flan_t5,\n",
    "    max_new_tokens=96,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "The **template** will be the same for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]\n",
    "You are a helpful, respectful and honest assistant.\n",
    "Always assist with care, respect, and truth. Respond with utmost utility yet securely.\n",
    "Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\n",
    "I will give you a text, then ask a question about it. Give a precise and as concise as possible answer to this question.\n",
    "\n",
    "### TEXT:\n",
    "{text}\n",
    "\n",
    "### QUESTION:\n",
    "{query}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "And we can now create two **conversation** objects that we will use to query the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )\n",
    "conversation_flant5 = LLMChain(llm=llm_flant5,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the models!\n",
    "\n",
    "In this example, we are only going to query one claim and see what happens. Of course, feel free to try with different ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************\n",
      "* Claim: claims/claim1.json\n",
      "***************************\n",
      "Original content:\n",
      "-----------------\n",
      "Subject: Claim for Recent Car Accident - Policy Number: AC-987654321\n",
      "Content:\n",
      "Dear Pacific Shield Insurance,\n",
      "\n",
      "I hope this email finds you well. My name is Sarah Turner, and I am writing to file a claim for a recent car accident that occurred on January 2nd, 2024, at approximately 3:30 PM. My policy number is AC-987654321.\n",
      "\n",
      "The accident took place at the intersection of Birch Street and Willow Avenue in the city of Evergreen. I was driving my vehicle, a black Toyota Camry with license plate number DEF-456, heading south on Birch Street. At the intersection, the traffic signal was green, and I proceeded through the intersection.\n",
      "\n",
      "At the same time, another vehicle, a blue Chevrolet Traverse with license plate number GHI-789, was traveling west on Willow Avenue. Unfortunately, the driver failed to stop at the red traffic signal, resulting in a collision with the front passenger side of my vehicle.\n",
      "\n",
      "The impact caused significant damage to both vehicles. The front bumper and right headlight of my Toyota Camry are extensively damaged, and there are also damages to the front driver's side of the Chevrolet Traverse. Fortunately, no injuries were sustained during the accident, and both drivers were able to move their vehicles to the side of the road.\n",
      "\n",
      "I promptly exchanged information with the other driver, Mr. Daniel Reynolds, including our names, phone numbers, insurance details, and a brief description of the accident. Additionally, I took photos of the accident scene, including the damages to both vehicles and the position of the traffic signal.\n",
      "\n",
      "I have attached the necessary documents to this email, including the photos, a copy of the police report filed at the Evergreen Police Department, and the estimate for the repair costs from Evergreen Auto Repair, where I have taken my vehicle for assessment.\n",
      "\n",
      "I kindly request your prompt attention to this matter and would appreciate any guidance on the next steps in the claims process. If you require any additional information or documentation, please do not hesitate to contact me at (555) 123-4567 or sarah.turner@email.com.\n",
      "\n",
      "Thank you for your assistance, and I look forward to a swift resolution of this claim.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Sarah Turner\n",
      "123 Oak Street\n",
      "Evergreen, CA 98765\n",
      "(555) 123-4567\n",
      "sarah.turner@email.com\n",
      "\n",
      "\n",
      "Analysis with Mistral-7B:\n",
      "--------\n",
      "- Sentiment: \n",
      "The sender, Sarah Turner, expresses a polite and professional tone in her email. She is respectful and detailed in her description of the car accident and the subsequent steps she took to file a claim. She requests prompt attention to the matter and provides all necessary documentation. Overall, her sentiment is one of being proactive and cooperative in the claims process.\n",
      "- Location: \n",
      "The car accident described in the claim occurred at the intersection of Birch Street and Willow Avenue in the city of Evergreen.\n",
      "- Time: \n",
      "The car accident referred to in the claim occurred on January 2nd, 2024, at approximately 3:30 PM.\n",
      "\n",
      "                          ----====----\n",
      "\n",
      "Analysis with Flan-T5-Small:\n",
      "--------\n",
      "- Sentiment: \n",
      " negative\n",
      "- Location: \n",
      " Evergreen Auto Repair\n",
      "- Time: \n",
      " a day\n",
      "\n",
      "                          ----====----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of URLs to fetch the JSON files from\n",
    "urls = [\n",
    "    'http://backend:8080/claim/claim1.json',\n",
    "    'http://backend:8080/claim/claim2.json',\n",
    "    'http://backend:8080/claim/claim3.json'\n",
    "]\n",
    "\n",
    "claims = {}\n",
    "\n",
    "for url in urls:\n",
    "    # Extract the filename from the URL\n",
    "    filename = os.path.basename(url)\n",
    "    \n",
    "    # Make an HTTP GET request to fetch the JSON file\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we notice bad responses\n",
    "    \n",
    "    # Load the JSON data from the response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Store the data in the claims dictionary\n",
    "    claims[filename] = data\n",
    "\n",
    "\n",
    "# Opening JSON file\n",
    "claims = {}\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "claims[filename] = data\n",
    "\n",
    "# Analyze the claim\n",
    "print(f\"***************************\")\n",
    "print(f\"* Claim: {filename}\")\n",
    "print(f\"***************************\")\n",
    "print(\"Original content:\")\n",
    "print(\"-----------------\")\n",
    "print(f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\\n\\n\")\n",
    "print('Analysis with Mistral-7B:')\n",
    "print(\"--------\")\n",
    "text_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"What is the sentiment of the person sending this claim?\"\n",
    "location_query = \"Where does the event the claim is related to happen?\"\n",
    "time_query = \"When does the event the claim is related to happen? If possible, specify the date and the time.\"\n",
    "print(f\"- Sentiment: \")\n",
    "conversation.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- Location: \")\n",
    "conversation.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- Time: \")\n",
    "conversation.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")\n",
    "print('Analysis with Flan-T5-Small:')\n",
    "print(\"--------\")\n",
    "text_input = f\"Subject: {claims[filename]['subject']}\\nContent:\\n{claims[filename]['content']}\"\n",
    "sentiment_query = \"What is the sentiment of the person sending this claim?\"\n",
    "location_query = \"Where does the event the claim is related to happen?\"\n",
    "time_query = \"When does the event the claim is related to happen? If possible, specify the date and the time.\"\n",
    "print(f\"- Sentiment: \")\n",
    "conversation_flant5.predict(text=text_input, query=sentiment_query);\n",
    "print(\"\\n- Location: \")\n",
    "conversation_flant5.predict(text=text_input, query=location_query);\n",
    "print(\"\\n- Time: \")\n",
    "conversation_flant5.predict(text=text_input, query=time_query);\n",
    "print(\"\\n\\n                          ----====----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28a5b0-6c93-42ba-84dd-42e17746d11d",
   "metadata": {},
   "source": [
    "As you can see, Flan-T5-Small is much faster. After all, it's an 80 Million parameters model only. It seems to work to some extent, but the results are nowhere near the ones from Mistral-7B, which is a 7 Billion parameter.\n",
    "\n",
    "The art of working with LLM is to find the right balance between the performance and accuracy you require, and the resources it takes along with the involved costs.\n",
    "\n",
    "Therefore it's important to have sanity checks in place to make sure that as you data changes, or your model evolves, you always get the behaviour you expected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
